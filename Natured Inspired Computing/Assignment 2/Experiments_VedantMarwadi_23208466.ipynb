{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7298eea-a6c9-48e4-abaf-9ba25afe4e15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"color: brown;\">1. Importing Libraries</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "92d044df-d8b7-4b12-8e23-a07d2cc788e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from deap import base, creator, tools, algorithms\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910c6ae",
   "metadata": {},
   "source": [
    "This notebook makes use of the DEAP library, which is essential for evolutionary algorithms. If it is not already installed, it can be added by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db0f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d77b22-0c85-4fd9-9fd1-bc6e3f992d05",
   "metadata": {},
   "source": [
    "<h1 style=\"color: brown;\">2. California Housing Prices</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb855ee-6bb3-4967-ae3d-10b14adc6731",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2 style=\"color: brown;\">2.1 Fetching and Preparing the California Housing Dataset</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85e65c5c-fb9d-462a-ab0f-11dfc110581c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetching the California housing dataset from the source\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Converting the dataset into a Pandas DataFrame for easier handling\n",
    "data = pd.DataFrame(data=california_housing.data, columns=california_housing.feature_names)\n",
    "\n",
    "# Adding a new column for house prices (target values) to the DataFrame\n",
    "data['Target'] = california_housing.target\n",
    "\n",
    "# Checking if there are any null values in the DataFrame\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7a8f4-b421-44bf-a0e7-814dba6d7301",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h2 style=\"color: brown;\">2.2 Preparing the Feature Set and Target Variable for Model Training</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ffc6a90-ed92-4e3a-9057-5233dc82e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'Target' column from the DataFrame to create the feature set (X)\n",
    "X = data.drop('Target', axis=1)\n",
    "\n",
    "# Extracting the 'Target' column from the DataFrame to create the target variable (y)\n",
    "y = data['Target']\n",
    "\n",
    "# Storing the names of the features (column names) from the original dataset\n",
    "feature_names = california_housing.feature_names\n",
    "\n",
    "# Creating an instance of StandardScaler to standardize the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting the scaler to the feature data and transforming the data to have a mean of 0 and a standard deviation of 1\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ee278-f64b-4dca-a764-460ea91e24c5",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">2.3 Evaluating Regression Models on the California Housing Dataset</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1201eb05-2bf3-4812-995d-f5c446562778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN MSE with All Features: 0.423345\n",
      "Decision Tree MSE with All Features: 0.531282\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary of different regression models to evaluate\n",
    "regressors = {\n",
    "    \"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "}\n",
    "\n",
    "# Initializing an empty dictionary to store Mean Squared Errors (MSE) for each model\n",
    "mse_all_features = {}\n",
    "\n",
    "# Looping through each regressor in the dictionary to evaluate its performance\n",
    "for name, model in regressors.items():\n",
    "    # Fitting the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting values using the test data\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculating the Mean Squared Error (MSE) of the predictions\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    # Storing the MSE for the current model in the dictionary\n",
    "    mse_all_features[name] = mse\n",
    "    \n",
    "    # Printing the MSE of the current model\n",
    "    print(f\"{name} MSE with All Features: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ae535-58e8-4ca3-977f-be9b51ccd027",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">2.4 Setting Up a Genetic Algorithm for Feature Selection with Regression Models</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48cc7db-ffde-4e70-92fe-da5132e50b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the fitness function for the genetic algorithm using a given regressor\n",
    "def evaluate(individual, model):\n",
    "    # Selecting features based on the individualâ€™s binary representation\n",
    "    selected_features = [index for index, bit in enumerate(individual) if bit]\n",
    "    \n",
    "    # Handling the case where no features are selected to avoid division by zero\n",
    "    if len(selected_features) == 0:\n",
    "        return (float('inf'),)  # Returning a high error value to indicate poor fitness\n",
    "    \n",
    "    # Creating a subset of features based on the selected features\n",
    "    X_subset = X[:, selected_features]\n",
    "    \n",
    "    # Splitting the feature subset and target data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Fitting the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions using the test data\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculating the Mean Squared Error (MSE) of the predictions\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    # Returning the MSE as the fitness value\n",
    "    return (mse,)\n",
    "\n",
    "# Setting up the Genetic Algorithm components\n",
    "\n",
    "# Creating the fitness function class with minimization (lower MSE is better)\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "\n",
    "# Creating the individual class with a fitness attribute\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "# Initializing the genetic algorithm toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Registering the attribute generator for individuals (binary values)\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
    "\n",
    "# Registering the individual generator (a list of binary values)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
    "\n",
    "# Registering the population generator (a list of individuals)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Registering the mating function (uniform crossover)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "\n",
    "# Registering the mutation function (flip bit mutation)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.2)\n",
    "\n",
    "# Registering the selection function (tournament selection)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab11dc-b905-4cda-8524-0f75497c93d0",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\"> 2.5 Running the Genetic Algorithm for Feature Selection with Different Regression Models</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e12edc73-3d6d-4f8c-bdb7-9015dbe53bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GA for KNN...\n",
      "Generation 1: MSE: 0.339875, Feature Subset: [0, 0, 1, 0, 0, 0, 1, 1]\n",
      "Generation 2: MSE: 0.302857, Feature Subset: [0, 0, 1, 0, 0, 1, 1, 1]\n",
      "Generation 3: MSE: 0.296418, Feature Subset: [1, 0, 0, 0, 0, 1, 1, 1]\n",
      "Generation 4: MSE: 0.296418, Feature Subset: [1, 0, 0, 0, 0, 1, 1, 1]\n",
      "Generation 5: MSE: 0.296418, Feature Subset: [1, 0, 0, 0, 0, 1, 1, 1]\n",
      "Generation 6: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 7: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 8: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 9: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 10: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 11: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 12: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 13: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 14: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 15: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 16: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 17: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 18: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 19: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 20: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 21: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 22: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 23: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 24: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 25: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 26: MSE: 0.282219, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "\n",
      "Running GA for Decision Tree...\n",
      "Generation 1: MSE: 0.429578, Feature Subset: [1, 0, 0, 1, 1, 0, 1, 1]\n",
      "Generation 2: MSE: 0.378900, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 3: MSE: 0.380600, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 4: MSE: 0.378163, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 5: MSE: 0.376986, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 6: MSE: 0.379169, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 7: MSE: 0.380728, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 8: MSE: 0.380172, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 9: MSE: 0.379069, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 10: MSE: 0.377424, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 11: MSE: 0.377697, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 12: MSE: 0.380456, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 13: MSE: 0.379866, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 14: MSE: 0.376411, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 15: MSE: 0.377455, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 16: MSE: 0.377545, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 17: MSE: 0.378981, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 18: MSE: 0.379369, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 19: MSE: 0.377715, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 20: MSE: 0.380991, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 21: MSE: 0.378263, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 22: MSE: 0.377701, Feature Subset: [0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Defining the function to run the Genetic Algorithm (GA) for a specific regressor\n",
    "def run_ga_for_regressor(regressor_name, model):\n",
    "    # Initializing the population of individuals for the GA\n",
    "    population = toolbox.population(n=100)\n",
    "\n",
    "    # Setting up a custom stopping condition\n",
    "    max_stable_generations = 20\n",
    "    stable_generations = 0\n",
    "    previous_best_individual = None\n",
    "    best_individual = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    generation = 0\n",
    "    while stable_generations < max_stable_generations:\n",
    "        generation += 1\n",
    "\n",
    "        # Registering the evaluation function with the chosen regressor\n",
    "        toolbox.register(\"evaluate\", evaluate, model=model)\n",
    "\n",
    "        # Running one generation of the genetic algorithm\n",
    "        population, _ = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=1, verbose=False)\n",
    "\n",
    "        # Extracting the best individual from the current generation\n",
    "        current_best_individual = tools.selBest(population, 1)[0]\n",
    "        current_best_mse = evaluate(current_best_individual, model)[0]  # Unpacking the tuple correctly\n",
    "\n",
    "        # Printing the results of the current best individual (binary vector)\n",
    "        print(f\"Generation {generation}: MSE: {current_best_mse:.6f}, Feature Subset: {current_best_individual}\")\n",
    "\n",
    "        # Checking if the current best individual is the same as the previous best\n",
    "        if previous_best_individual is not None and current_best_individual == previous_best_individual:\n",
    "            stable_generations += 1\n",
    "        else:\n",
    "            stable_generations = 0\n",
    "            previous_best_individual = current_best_individual\n",
    "\n",
    "        # Updating the best individual if necessary\n",
    "        if current_best_mse < best_mse:\n",
    "            best_individual = current_best_individual\n",
    "            best_mse = current_best_mse\n",
    "\n",
    "    # Determining the feature names of the best individual from the final generation\n",
    "    final_selected_features = [i for i, bit in enumerate(best_individual) if bit]\n",
    "    final_selected_feature_names = [feature_names[i] for i in final_selected_features]\n",
    "    final_not_selected_feature_names = [feature_names[i] for i in range(len(feature_names)) if i not in final_selected_features]\n",
    "\n",
    "    return best_individual, best_mse, final_selected_features, final_selected_feature_names, final_not_selected_feature_names\n",
    "\n",
    "# Initializing dictionaries to store results for GA-selected features\n",
    "mse_ga_features = {}\n",
    "selected_features_ga = {}\n",
    "selected_features_names_ga = {}\n",
    "not_selected_features_names_ga = {}\n",
    "\n",
    "# Running the GA for each regressor in the regressors dictionary\n",
    "for name, model in regressors.items():\n",
    "    print(f\"\\nRunning GA for {name}...\")\n",
    "    best_individual, best_mse, selected_features, selected_feature_names, not_selected_feature_names = run_ga_for_regressor(name, model)\n",
    "    \n",
    "    # Storing the results for each regressor\n",
    "    mse_ga_features[name] = best_mse\n",
    "    selected_features_ga[name] = selected_features\n",
    "    selected_features_names_ga[name] = selected_feature_names\n",
    "    not_selected_features_names_ga[name] = not_selected_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f8c6cd-26e7-4a2a-8304-a352e7c09d1e",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">2.6 Final Comparison of MSEs with All Features vs. GA-Selected Features</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8df22f0d-53d6-4794-899b-0ed405f97956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of MSE with All Features vs GA-Selected Features:\n",
      "KNN: MSE with All Features: 0.423345 | MSE with GA-Selected Features: 0.282219\n",
      "Decision Tree: MSE with All Features: 0.531282 | MSE with GA-Selected Features: 0.376411\n"
     ]
    }
   ],
   "source": [
    "# Printing the final comparison of Mean Squared Errors (MSEs) with all features versus GA-selected features\n",
    "print(\"\\nComparison of MSE with All Features vs GA-Selected Features:\")\n",
    "\n",
    "# Looping through each regressor to compare the MSEs\n",
    "for name in regressors.keys():\n",
    "    print(f\"{name}: MSE with All Features: {mse_all_features[name]:.6f} | MSE with GA-Selected Features: {mse_ga_features[name]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92812871-477c-43c6-b2e7-4512d389723e",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">2.7 Displaying and Tracking Feature Selections After GA Optimization</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1326c8-9a7e-444c-8695-76ebfd178f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features After GA Optimization:\n",
      "KNN:\n",
      "  Features to Select: [6, 7]\n",
      "  Feature names: ['Latitude', 'Longitude']\n",
      "  Not Selected Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
      "Decision Tree:\n",
      "  Features to Select: [6, 7]\n",
      "  Feature names: ['Latitude', 'Longitude']\n",
      "  Not Selected Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n"
     ]
    }
   ],
   "source": [
    "# Initializing counters to keep track of selected and not selected features\n",
    "selected_feature_counter = Counter()\n",
    "not_selected_feature_counter = Counter()\n",
    "\n",
    "# Printing the final feature selections and non-selections after GA optimization\n",
    "print(\"\\nSelected Features After GA Optimization:\")\n",
    "for name in regressors.keys():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Features to Select: {selected_features_ga[name]}\")\n",
    "    print(f\"  Feature names: {selected_features_names_ga[name]}\")\n",
    "    print(f\"  Not Selected Features: {not_selected_features_names_ga[name]}\")\n",
    "\n",
    "    # Updating the counters with the selected and not selected feature names\n",
    "    selected_feature_counter.update(selected_features_names_ga[name])\n",
    "    not_selected_feature_counter.update(not_selected_features_names_ga[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8aa11a-5e2b-4b4a-be31-a68c6b008f4e",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">2.8 Analyzing Feature Selection Frequency After GA Optimization</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7de71e1-ee96-45e9-9422-6f2805a1d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "Selected 2 times: ['Latitude', 'Longitude']\n",
      "Selected 1 time: []\n",
      "\n",
      "Feature Non-Selection Frequency:\n",
      "Not Selected 2 times: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
      "Not Selected 1 time: []\n"
     ]
    }
   ],
   "source": [
    "# Categorizing the selected features based on how often they are selected\n",
    "selected_2_times = [feature for feature, count in selected_feature_counter.items() if count == 2]\n",
    "selected_1_time = [feature for feature, count in selected_feature_counter.items() if count == 1]\n",
    "\n",
    "# Categorizing the not selected features based on how often they are not selected\n",
    "not_selected_2_times = [feature for feature, count in not_selected_feature_counter.items() if count == 2]\n",
    "not_selected_1_time = [feature for feature, count in not_selected_feature_counter.items() if count == 1]\n",
    "\n",
    "# Printing the categorized results of feature selection frequency\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(f\"Selected 2 times: {selected_2_times}\")\n",
    "print(f\"Selected 1 time: {selected_1_time}\")\n",
    "\n",
    "# Printing the categorized results of feature non-selection frequency\n",
    "print(\"\\nFeature Non-Selection Frequency:\")\n",
    "print(f\"Not Selected 2 times: {not_selected_2_times}\")\n",
    "print(f\"Not Selected 1 time: {not_selected_1_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263a5e2-5832-4e72-be49-2517cc795146",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"color: brown;\">3. Student Performance</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786c73a-1bc7-4340-8168-6c4a515e41e8",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.1 Analyzing and Displaying Column Types in the Student Dataset </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "120b5a52-ea68-44de-89eb-d3fe9ad0a9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numerical columns: 16\n",
      "Numerical columns: ['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
      "\n",
      "Number of categorical columns: 17\n",
      "Categorical columns: ['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']\n"
     ]
    }
   ],
   "source": [
    "# Reading the CSV file into a DataFrame\n",
    "data = pd.read_csv(\"student.csv\")\n",
    "\n",
    "# Selecting columns with numerical data types\n",
    "numerical_cols = data.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Selecting columns with categorical data types\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Counting the number of numerical columns\n",
    "num_numerical = len(numerical_cols)\n",
    "\n",
    "# Counting the number of categorical columns\n",
    "num_categorical = len(categorical_cols)\n",
    "\n",
    "# Printing the number of numerical columns and listing them\n",
    "print(f'Number of numerical columns: {num_numerical}')\n",
    "print('Numerical columns:', list(numerical_cols))\n",
    "print()\n",
    "\n",
    "# Printing the number of categorical columns and listing them\n",
    "print(f'Number of categorical columns: {num_categorical}')\n",
    "print('Categorical columns:', list(categorical_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e03cc0c-b22c-488f-b295-7bd783d42c59",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.2 Dropping Specific Columns from the Dataset</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c02c6df-3ed2-4b4f-af57-deede7c6f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the 'G1' and 'G2' columns from the DataFrame\n",
    "data = data.drop(columns=['G1', 'G2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6aea7-c483-4994-b729-e2c11609190a",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.3 Encoding Categorical Features</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19848ec8-b736-4e02-ad23-3662d6648be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding Applied:\n",
      "Column 'school': Label Encoding\n",
      "Column 'sex': Label Encoding\n",
      "Column 'address': Label Encoding\n",
      "Column 'famsize': Label Encoding\n",
      "Column 'Pstatus': Label Encoding\n",
      "Column 'Mjob': One-Hot Encoding\n",
      "Column 'Fjob': One-Hot Encoding\n",
      "Column 'reason': One-Hot Encoding\n",
      "Column 'guardian': One-Hot Encoding\n",
      "Column 'schoolsup': Label Encoding\n",
      "Column 'famsup': Label Encoding\n",
      "Column 'paid': Label Encoding\n",
      "Column 'activities': Label Encoding\n",
      "Column 'nursery': Label Encoding\n",
      "Column 'higher': Label Encoding\n",
      "Column 'internet': Label Encoding\n",
      "Column 'romantic': Label Encoding\n"
     ]
    }
   ],
   "source": [
    "# Initializing dictionaries and lists to store encoding information\n",
    "encoding_info = {}\n",
    "label_encode_cols = []\n",
    "one_hot_encode_cols = []\n",
    "\n",
    "# Looping through each categorical column to determine the encoding method\n",
    "for col in categorical_cols:\n",
    "    # Counting the number of unique values in the column\n",
    "    num_unique = data[col].nunique()\n",
    "    \n",
    "    # Choosing encoding method based on the number of unique values\n",
    "    if num_unique <= 2:\n",
    "        label_encode_cols.append(col)  # Adding column to label encoding list\n",
    "        encoding_info[col] = \"Label Encoding\"  # Recording encoding method\n",
    "    else:\n",
    "        one_hot_encode_cols.append(col)  # Adding column to one-hot encoding list\n",
    "        encoding_info[col] = \"One-Hot Encoding\"  # Recording encoding method\n",
    "\n",
    "# Applying Label Encoding to the appropriate columns\n",
    "le = LabelEncoder()\n",
    "for col in label_encode_cols:\n",
    "    data[col] = le.fit_transform(data[col])  # Transforming the column values\n",
    "\n",
    "# Applying One-Hot Encoding to the appropriate columns\n",
    "data = pd.get_dummies(data, columns=one_hot_encode_cols)\n",
    "\n",
    "# Printing the encoding methods applied to each column\n",
    "print(\"\\nEncoding Applied:\")\n",
    "for col, encoding in encoding_info.items():\n",
    "    print(f\"Column '{col}': {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef6aea-3e1b-44f8-9fa5-6f42eb81e981",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.4 Splitting the Student Dataset into Features and Target Variable</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e9be535-b32f-4c5a-bf1c-3642ee85d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the features and the target variable from the DataFrame\n",
    "X = data.drop('G3', axis=1)  # Features: All columns except 'G3'\n",
    "y = data['G3']  # Target variable: 'G3'\n",
    "\n",
    "# Getting the list of feature names from the DataFrame\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Splitting the feature data (X) and target data (y) into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a63bf-6050-41c8-b706-0651ca76f062",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.5 Evaluating Regression Models on the Student Performance Dataset</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303cd4a6-e555-4c39-ba12-fcec938c506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN MSE with All Features: 19.972773\n",
      "Decision Tree MSE with All Features: 32.344538\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of regressors to evaluate\n",
    "regressors = {\n",
    "    \"KNN\": KNeighborsRegressor(n_neighbors=5),  # K-Nearest Neighbors Regressor\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),  # Decision Tree Regressor\n",
    "}\n",
    "\n",
    "# Initializing a dictionary to store Mean Squared Errors (MSE) for each regressor\n",
    "mse_all_features = {}\n",
    "\n",
    "# Looping through each regressor to evaluate its performance\n",
    "for name, model in regressors.items():\n",
    "    # Fitting the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions using the test data\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculating the Mean Squared Error (MSE) of the predictions\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    # Storing the MSE for the current model in the dictionary\n",
    "    mse_all_features[name] = mse\n",
    "    \n",
    "    # Printing the MSE of the current model with all features\n",
    "    print(f\"{name} MSE with All Features: {mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b775d-8b6e-45fe-87e5-45d07f6d09df",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.6 Setting Up the Genetic Algorithm for Feature Selection</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3591c60d-1d10-4c75-90e6-47b244670236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to evaluate the performance of an individual (feature subset) using a given model\n",
    "def evaluate(individual, model):\n",
    "    # Selecting features based on the binary representation of the individual\n",
    "    selected_features = [i for i, bit in enumerate(individual) if bit]\n",
    "    \n",
    "    # Handling the case where no features are selected to avoid division by zero\n",
    "    if len(selected_features) == 0:\n",
    "        return (float('inf'),)  # Returning a high error value to indicate poor fitness\n",
    "\n",
    "    # Creating a subset of the features based on the selected features\n",
    "    X_subset = X.iloc[:, selected_features]\n",
    "\n",
    "    # Splitting the feature subset and target data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Fitting the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions using the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculating the Mean Squared Error (MSE) of the predictions\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Returning the MSE as the fitness value\n",
    "    return (mse,)\n",
    "\n",
    "# Setting up the Genetic Algorithm (GA) components\n",
    "\n",
    "# Creating a fitness function class with minimization (lower MSE is better)\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "\n",
    "# Creating the individual class with a fitness attribute\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "# Initializing the GA toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Registering the attribute generator for individuals (binary values)\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
    "\n",
    "# Registering the individual generator (a list of binary values)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
    "\n",
    "# Registering the population generator (a list of individuals)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Registering the mating function (uniform crossover)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "\n",
    "# Registering the mutation function (flip bit mutation)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.2)\n",
    "\n",
    "# Registering the selection function (tournament selection)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc0a18-0d7d-478f-8488-448dc742e424",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.7 Running Genetic Algorithm for Feature Selection and Storing Results</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afb1bab8-da83-47db-8bb9-18ebb542d7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GA for KNN...\n",
      "Generation 1: MSE: 17.785210, Feature Subset: [0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1]\n",
      "Generation 2: MSE: 16.245378, Feature Subset: [0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1]\n",
      "Generation 3: MSE: 16.564034, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "Generation 4: MSE: 16.468235, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "Generation 5: MSE: 15.796975, Feature Subset: [0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 6: MSE: 15.796975, Feature Subset: [0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 7: MSE: 15.365378, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "Generation 8: MSE: 14.875294, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 9: MSE: 14.875294, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 10: MSE: 14.875294, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 11: MSE: 14.804370, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 12: MSE: 14.401345, Feature Subset: [0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "Generation 13: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 14: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 15: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 16: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 17: MSE: 14.351261, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1]\n",
      "Generation 18: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 19: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 20: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 21: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 22: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 23: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 24: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 25: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 26: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 27: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 28: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 29: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 30: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 31: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 32: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 33: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 34: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 35: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 36: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 37: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "Generation 38: MSE: 14.545210, Feature Subset: [0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "\n",
      "Running GA for Decision Tree...\n",
      "Generation 1: MSE: 26.210084, Feature Subset: [0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 2: MSE: 23.296218, Feature Subset: [1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n",
      "Generation 3: MSE: 25.521008, Feature Subset: [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
      "Generation 4: MSE: 22.462185, Feature Subset: [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
      "Generation 5: MSE: 22.268908, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1]\n",
      "Generation 6: MSE: 19.117647, Feature Subset: [1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "Generation 7: MSE: 23.084034, Feature Subset: [0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "Generation 8: MSE: 21.991597, Feature Subset: [1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1]\n",
      "Generation 9: MSE: 23.707983, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 10: MSE: 15.894958, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 11: MSE: 15.936975, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 12: MSE: 15.810924, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 13: MSE: 17.315126, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 14: MSE: 17.394958, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 15: MSE: 15.909664, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 16: MSE: 16.159664, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 17: MSE: 16.063025, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 18: MSE: 14.943277, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "Generation 19: MSE: 14.665966, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "Generation 20: MSE: 14.659664, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 21: MSE: 15.205882, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 22: MSE: 13.607143, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 23: MSE: 13.676471, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 24: MSE: 16.144958, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 25: MSE: 13.817227, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 26: MSE: 15.850840, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 27: MSE: 16.699580, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 28: MSE: 14.701681, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 29: MSE: 16.405462, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 30: MSE: 15.304622, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 31: MSE: 13.886555, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 32: MSE: 14.096639, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 33: MSE: 16.567227, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 34: MSE: 14.758403, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 35: MSE: 15.581933, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 36: MSE: 16.363445, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 37: MSE: 16.649160, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 38: MSE: 15.565126, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 39: MSE: 14.852941, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 40: MSE: 13.573529, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 41: MSE: 15.598739, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 42: MSE: 13.289916, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 43: MSE: 13.909664, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 44: MSE: 15.970588, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 45: MSE: 14.750000, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 46: MSE: 14.817227, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 47: MSE: 14.441176, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 48: MSE: 14.556723, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 49: MSE: 13.750000, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 50: MSE: 13.254202, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 51: MSE: 16.134454, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 52: MSE: 14.220588, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 53: MSE: 15.993697, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 54: MSE: 13.010504, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 55: MSE: 15.186975, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 56: MSE: 16.441176, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 57: MSE: 14.539916, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 58: MSE: 13.338235, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 59: MSE: 14.363445, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 60: MSE: 14.539916, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 61: MSE: 13.827731, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 62: MSE: 14.027311, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 63: MSE: 15.640756, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 64: MSE: 14.144958, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 65: MSE: 14.180672, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 66: MSE: 14.657563, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 67: MSE: 15.371849, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 68: MSE: 12.878151, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 69: MSE: 15.117647, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 70: MSE: 15.021008, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 71: MSE: 13.934874, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 72: MSE: 13.373950, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 73: MSE: 14.707983, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 74: MSE: 15.271008, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 75: MSE: 12.659664, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 76: MSE: 15.388655, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 77: MSE: 15.581933, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "Generation 78: MSE: 14.884454, Feature Subset: [1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Defining the function to run the Genetic Algorithm (GA) for a specific regressor\n",
    "def run_ga_for_regressor(regressor_name, model):\n",
    "    # Initializing the population of individuals for the GA\n",
    "    population = toolbox.population(n=100)\n",
    "\n",
    "    # Setting up a custom stopping condition\n",
    "    max_stable_generations = 20\n",
    "    stable_generations = 0\n",
    "    previous_best_individual = None\n",
    "    best_individual = None\n",
    "    best_mse = float('inf')\n",
    "\n",
    "    generation = 0\n",
    "    while stable_generations < max_stable_generations:\n",
    "        generation += 1\n",
    "\n",
    "        # Registering the evaluation function with the chosen regressor\n",
    "        toolbox.register(\"evaluate\", evaluate, model=model)\n",
    "\n",
    "        # Running one generation of the genetic algorithm\n",
    "        population, _ = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=1, verbose=False)\n",
    "\n",
    "        # Extracting the best individual from the current generation\n",
    "        current_best_individual = tools.selBest(population, 1)[0]\n",
    "        current_best_mse = evaluate(current_best_individual, model)[0]  # Unpacking the tuple correctly\n",
    "\n",
    "        # Printing the results of the current best individual (binary vector)\n",
    "        print(f\"Generation {generation}: MSE: {current_best_mse:.6f}, Feature Subset: {current_best_individual}\")\n",
    "\n",
    "        # Checking if the current best individual is the same as the previous best\n",
    "        if previous_best_individual is not None and current_best_individual == previous_best_individual:\n",
    "            stable_generations += 1\n",
    "        else:\n",
    "            stable_generations = 0\n",
    "            previous_best_individual = current_best_individual\n",
    "\n",
    "        # Updating the best individual if necessary\n",
    "        if current_best_mse < best_mse:\n",
    "            best_individual = current_best_individual\n",
    "            best_mse = current_best_mse\n",
    "\n",
    "    # Determining the feature names of the best individual from the final generation\n",
    "    final_selected_features = [i for i, bit in enumerate(best_individual) if bit]\n",
    "    final_selected_feature_names = [feature_names[i] for i in final_selected_features]\n",
    "    final_not_selected_feature_names = [feature_names[i] for i in range(len(feature_names)) if i not in final_selected_features]\n",
    "\n",
    "    return best_individual, best_mse, final_selected_features, final_selected_feature_names, final_not_selected_feature_names\n",
    "\n",
    "# Initializing dictionaries to store results for GA-selected features\n",
    "mse_ga_features = {}\n",
    "selected_features_ga = {}\n",
    "selected_features_names_ga = {}\n",
    "not_selected_features_names_ga = {}\n",
    "\n",
    "# Running the GA for each regressor in the regressors dictionary\n",
    "for name, model in regressors.items():\n",
    "    print(f\"\\nRunning GA for {name}...\")\n",
    "    best_individual, best_mse, selected_features, selected_feature_names, not_selected_feature_names = run_ga_for_regressor(name, model)\n",
    "    \n",
    "    # Storing the results for each regressor\n",
    "    mse_ga_features[name] = best_mse\n",
    "    selected_features_ga[name] = selected_features\n",
    "    selected_features_names_ga[name] = selected_feature_names\n",
    "    not_selected_features_names_ga[name] = not_selected_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a798b-4983-4a10-b073-a969055a18a2",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.8 Final Comparison of MSEs with All Features vs. GA-Selected Features</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "520aa7b1-b08e-4966-87f9-ad2ad93964d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of MSE with All Features vs GA-Selected Features:\n",
      "KNN: MSE with All Features: 19.972773 | MSE with GA-Selected Features: 14.351261\n",
      "Decision Tree: MSE with All Features: 32.344538 | MSE with GA-Selected Features: 12.659664\n"
     ]
    }
   ],
   "source": [
    "# Printing the final comparison of Mean Squared Errors (MSEs) with all features versus GA-selected features\n",
    "print(\"\\nComparison of MSE with All Features vs GA-Selected Features:\")\n",
    "\n",
    "# Looping through each regressor to compare the MSEs\n",
    "for name in regressors.keys():\n",
    "    print(f\"{name}: MSE with All Features: {mse_all_features[name]:.6f} | MSE with GA-Selected Features: {mse_ga_features[name]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dccb87-c1a8-4a58-b50c-19274345e0f7",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.9 Displaying and Tracking Feature Selections After GA Optimization</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3149e8ed-8494-43e8-85bb-e49b7ad668b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features After GA Optimization:\n",
      "KNN:\n",
      "  Features to Select: [1, 2, 4, 5, 6, 10, 11, 12, 15, 20, 21, 23, 24, 25, 26, 30, 31, 33, 35, 36, 40, 41, 42]\n",
      "  Feature names: ['sex', 'age', 'famsize', 'Pstatus', 'Medu', 'failures', 'schoolsup', 'famsup', 'nursery', 'freetime', 'goout', 'Walc', 'health', 'absences', 'Mjob_at_home', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_other', 'Fjob_teacher', 'reason_course', 'guardian_father', 'guardian_mother', 'guardian_other']\n",
      "  Not Selected Features: ['school', 'address', 'Fedu', 'traveltime', 'studytime', 'paid', 'activities', 'higher', 'internet', 'romantic', 'famrel', 'Dalc', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Fjob_health', 'Fjob_services', 'reason_home', 'reason_other', 'reason_reputation']\n",
      "Decision Tree:\n",
      "  Features to Select: [0, 1, 3, 5, 8, 9, 10, 11, 13, 14, 15, 17, 21, 23, 25, 27, 28, 30, 33, 34, 38, 39, 40, 42]\n",
      "  Feature names: ['school', 'sex', 'address', 'Pstatus', 'traveltime', 'studytime', 'failures', 'schoolsup', 'paid', 'activities', 'nursery', 'internet', 'goout', 'Walc', 'absences', 'Mjob_health', 'Mjob_other', 'Mjob_teacher', 'Fjob_other', 'Fjob_services', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_other']\n",
      "  Not Selected Features: ['age', 'famsize', 'Medu', 'Fedu', 'famsup', 'higher', 'romantic', 'famrel', 'freetime', 'Dalc', 'health', 'Mjob_at_home', 'Mjob_services', 'Fjob_at_home', 'Fjob_health', 'Fjob_teacher', 'reason_course', 'reason_home', 'guardian_mother']\n"
     ]
    }
   ],
   "source": [
    "# Initializing counters to keep track of selected and not selected features\n",
    "selected_feature_counter = Counter()\n",
    "not_selected_feature_counter = Counter()\n",
    "\n",
    "# Printing the final feature selections and non-selections after GA optimization\n",
    "print(\"\\nSelected Features After GA Optimization:\")\n",
    "for name in regressors.keys():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Features to Select: {selected_features_ga[name]}\")\n",
    "    print(f\"  Feature names: {selected_features_names_ga[name]}\")\n",
    "    print(f\"  Not Selected Features: {not_selected_features_names_ga[name]}\")\n",
    "\n",
    "    # Updating the counters with the selected and not selected feature names\n",
    "    selected_feature_counter.update(selected_features_names_ga[name])\n",
    "    not_selected_feature_counter.update(not_selected_features_names_ga[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b054e40-75ed-4395-aa73-b06155debcd5",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">3.10 Analyzing Feature Selection Frequency After GA Optimization</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97382de9-1d02-49cd-b5de-93b7e7aa1627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "Selected 2 times: ['sex', 'Pstatus', 'failures', 'schoolsup', 'nursery', 'goout', 'Walc', 'absences', 'Mjob_teacher', 'Fjob_other', 'guardian_father', 'guardian_other']\n",
      "Selected 1 time: ['age', 'famsize', 'Medu', 'famsup', 'freetime', 'health', 'Mjob_at_home', 'Fjob_at_home', 'Fjob_teacher', 'reason_course', 'guardian_mother', 'school', 'address', 'traveltime', 'studytime', 'paid', 'activities', 'internet', 'Mjob_health', 'Mjob_other', 'Fjob_services', 'reason_other', 'reason_reputation']\n",
      "\n",
      "Feature Non-Selection Frequency:\n",
      "Not Selected 2 times: ['Fedu', 'higher', 'romantic', 'famrel', 'Dalc', 'Mjob_services', 'Fjob_health', 'reason_home']\n",
      "Not Selected 1 time: ['school', 'address', 'traveltime', 'studytime', 'paid', 'activities', 'internet', 'Mjob_health', 'Mjob_other', 'Fjob_services', 'reason_other', 'reason_reputation', 'age', 'famsize', 'Medu', 'famsup', 'freetime', 'health', 'Mjob_at_home', 'Fjob_at_home', 'Fjob_teacher', 'reason_course', 'guardian_mother']\n"
     ]
    }
   ],
   "source": [
    "# Categorizing the selected features based on how often they are selected\n",
    "selected_2_times = [feature for feature, count in selected_feature_counter.items() if count == 2]\n",
    "selected_1_time = [feature for feature, count in selected_feature_counter.items() if count == 1]\n",
    "\n",
    "# Categorizing the not selected features based on how often they are not selected\n",
    "not_selected_2_times = [feature for feature, count in not_selected_feature_counter.items() if count == 2]\n",
    "not_selected_1_time = [feature for feature, count in not_selected_feature_counter.items() if count == 1]\n",
    "\n",
    "# Printing the categorized results of feature selection frequency\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(f\"Selected 2 times: {selected_2_times}\")\n",
    "print(f\"Selected 1 time: {selected_1_time}\")\n",
    "\n",
    "# Printing the categorized results of feature non-selection frequency\n",
    "print(\"\\nFeature Non-Selection Frequency:\")\n",
    "print(f\"Not Selected 2 times: {not_selected_2_times}\")\n",
    "print(f\"Not Selected 1 time: {not_selected_1_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6765e1b-0325-4946-aef7-24025016f09f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1 style=\"color: brown;\">4. Student Dropout and Academic Success</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14efff3-0195-4120-ab81-ca88a5711240",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">4.1 Loading Dataset and Analyzing Column Types</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7fa49d1f-203f-48f4-a537-d09e5322f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numerical columns: 36\n",
      "Numerical columns: ['Marital status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance\\t', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\", \"Father's occupation\", 'Admission grade', 'Displaced', 'Educational special needs', 'Debtor', 'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)', 'Unemployment rate', 'Inflation rate', 'GDP']\n",
      "\n",
      "Number of categorical columns: 1\n",
      "Categorical columns: ['Target']\n"
     ]
    }
   ],
   "source": [
    "# Reading the CSV file into a DataFrame\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Selecting columns with numerical data types\n",
    "numerical_cols = data.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Selecting columns with categorical data types\n",
    "categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Counting the number of numerical columns\n",
    "num_numerical = len(numerical_cols)\n",
    "\n",
    "# Counting the number of categorical columns\n",
    "num_categorical = len(categorical_cols)\n",
    "\n",
    "# Printing the number of numerical columns and listing them\n",
    "print(f'Number of numerical columns: {num_numerical}')\n",
    "print('Numerical columns:', list(numerical_cols))\n",
    "print()\n",
    "\n",
    "# Printing the number of categorical columns and listing them\n",
    "print(f'Number of categorical columns: {num_categorical}')\n",
    "print('Categorical columns:', list(categorical_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d805013-cfb8-48f9-9d6f-9c2a802f5ccd",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">4.2 Label Encoding the 'Target' Column in the Dataset</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a5834686-3a9c-4e18-8d7b-85e9a907d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column \"Target\" encoding:\n",
      "  Value: Dropout -> Encoded: 0\n",
      "  Value: Enrolled -> Encoded: 1\n",
      "  Value: Graduate -> Encoded: 2\n"
     ]
    }
   ],
   "source": [
    "# Initializing a LabelEncoder for encoding the 'Target' column\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fitting and transforming the 'Target' column with the LabelEncoder\n",
    "data['Target'] = label_encoder.fit_transform(data['Target'])\n",
    "\n",
    "# Creating a mapping of original values to encoded values for the 'Target' column\n",
    "mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# Printing the encoding mappings for the 'Target' column\n",
    "print('Column \"Target\" encoding:')\n",
    "for original, encoded in mapping.items():\n",
    "    print(f'  Value: {original} -> Encoded: {encoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8283423a-0f46-4d69-99c9-6e9daaff78bd",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">4.3 Preparing the Dataset for Model Training and Testing</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10551627-0bcb-4d0c-9e53-04fd9d9d3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the features and the target variable from the DataFrame\n",
    "X = data.drop('Target', axis=1)  # Features: All columns except 'Target'\n",
    "Y = data['Target']  # Target variable: 'Target'\n",
    "\n",
    "# Getting the list of feature names from the DataFrame\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Standardizing the features to have mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Splitting the feature data (X) and target data (Y) into training and testing sets\n",
    "# 30% of the data is used for testing, and the random state ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e9d92e-d384-4488-b42f-500666c8193f",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">4.4 Evaluating Classifier Models on the Dataset Using Accuracy</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c7cfbd3-10a8-4678-a434-978dd08598b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy with All Features: 0.691265\n",
      "Decision Tree Accuracy with All Features: 0.663404\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary of classifiers to evaluate\n",
    "classifiers = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Initializing a dictionary to store the accuracy of each classifier\n",
    "accuracy_all_features = {}\n",
    "\n",
    "# Looping through each classifier to evaluate its performance\n",
    "for name, model in classifiers.items():\n",
    "    # Fitting the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions using the test data\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculating the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Storing the accuracy in the dictionary\n",
    "    accuracy_all_features[name] = accuracy\n",
    "    \n",
    "    # Printing the accuracy of the current classifier with all features\n",
    "    print(f\"{name} Accuracy with All Features: {accuracy:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943011fd-105d-4f69-a832-a771badb95d8",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">4.5 Setting Up the Genetic Algorithm for Feature Selection with Classifier Models</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8d5437e-d788-4374-a783-25bd304b7400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vedan\\anaconda3\\envs\\comp815\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMax' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "C:\\Users\\vedan\\anaconda3\\envs\\comp815\\lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    }
   ],
   "source": [
    "# Defining the fitness function for the genetic algorithm using a given classifier\n",
    "def evaluate(individual, model):\n",
    "    # Selecting features based on the binary representation of the individual\n",
    "    selected_features = [index for index, bit in enumerate(individual) if bit]\n",
    "    \n",
    "    # Handling the case where no features are selected by returning a low accuracy value\n",
    "    if len(selected_features) == 0:\n",
    "        return (0.0,)  # Returning a low accuracy value to indicate poor fitness\n",
    "    \n",
    "    # Creating a subset of the features based on the selected features\n",
    "    X_subset = X[:, selected_features]\n",
    "    \n",
    "    # Splitting the feature subset and target data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_subset, Y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Fitting the model with the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Making predictions using the test data\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculating the accuracy of the predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Returning the accuracy as the fitness value\n",
    "    return (accuracy,)\n",
    "\n",
    "# Setting up the Genetic Algorithm (GA) components\n",
    "\n",
    "# Creating a fitness function class with maximization (higher accuracy is better)\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "\n",
    "# Creating the individual class with a fitness attribute\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Initializing the GA toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Registering the attribute generator for individuals (binary values)\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 2)\n",
    "\n",
    "# Registering the individual generator (a list of binary values)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=X.shape[1])\n",
    "\n",
    "# Registering the population generator (a list of individuals)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Registering the mating function (uniform crossover)\n",
    "toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "\n",
    "# Registering the mutation function (flip bit mutation)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.2)\n",
    "\n",
    "# Registering the selection function (tournament selection)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579f454-a33d-4f0e-ab01-2f79325c1a46",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">4.6 Running Genetic Algorithm for Feature Selection and Storing Results for Classifiers</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0773bb6c-640b-4e5a-be13-dcb05becd715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GA for KNN...\n",
      "Generation 1: Accuracy: 0.741717, Feature Subset: [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 2: Accuracy: 0.741717, Feature Subset: [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 3: Accuracy: 0.741717, Feature Subset: [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 4: Accuracy: 0.743223, Feature Subset: [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 5: Accuracy: 0.743223, Feature Subset: [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 6: Accuracy: 0.743976, Feature Subset: [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 7: Accuracy: 0.743976, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Generation 8: Accuracy: 0.743976, Feature Subset: [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 9: Accuracy: 0.746235, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 10: Accuracy: 0.746235, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 11: Accuracy: 0.751506, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Generation 12: Accuracy: 0.753012, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 13: Accuracy: 0.752259, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 14: Accuracy: 0.753765, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Generation 15: Accuracy: 0.753765, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Generation 16: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 17: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 18: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 19: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 20: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 21: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 22: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 23: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 24: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 25: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 26: Accuracy: 0.756024, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1]\n",
      "Generation 27: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 28: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 29: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 30: Accuracy: 0.755271, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 31: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 32: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 33: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 34: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 35: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 36: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 37: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 38: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 39: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 40: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 41: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 42: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 43: Accuracy: 0.761295, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 44: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 45: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 46: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 47: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 48: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 49: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 50: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 51: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 52: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 53: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 54: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 55: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 56: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 57: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 58: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 59: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 60: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 61: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 62: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 63: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "Generation 64: Accuracy: 0.762048, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n",
      "\n",
      "Running GA for Decision Tree...\n",
      "Generation 1: Accuracy: 0.689006, Feature Subset: [1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "Generation 2: Accuracy: 0.693524, Feature Subset: [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "Generation 3: Accuracy: 0.711596, Feature Subset: [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "Generation 4: Accuracy: 0.710843, Feature Subset: [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Generation 5: Accuracy: 0.714608, Feature Subset: [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "Generation 6: Accuracy: 0.715361, Feature Subset: [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "Generation 7: Accuracy: 0.717620, Feature Subset: [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "Generation 8: Accuracy: 0.716114, Feature Subset: [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Generation 9: Accuracy: 0.716114, Feature Subset: [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Generation 10: Accuracy: 0.713855, Feature Subset: [1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "Generation 11: Accuracy: 0.710090, Feature Subset: [1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 12: Accuracy: 0.710090, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 13: Accuracy: 0.715361, Feature Subset: [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Generation 14: Accuracy: 0.719127, Feature Subset: [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Generation 15: Accuracy: 0.716114, Feature Subset: [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Generation 16: Accuracy: 0.715361, Feature Subset: [1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "Generation 17: Accuracy: 0.718373, Feature Subset: [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Generation 18: Accuracy: 0.708584, Feature Subset: [0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "Generation 19: Accuracy: 0.712349, Feature Subset: [0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "Generation 20: Accuracy: 0.719880, Feature Subset: [0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "Generation 21: Accuracy: 0.730422, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 22: Accuracy: 0.732681, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 23: Accuracy: 0.730422, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 24: Accuracy: 0.729669, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 25: Accuracy: 0.729669, Feature Subset: [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 26: Accuracy: 0.723645, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 27: Accuracy: 0.734187, Feature Subset: [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 28: Accuracy: 0.724398, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 29: Accuracy: 0.722892, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 30: Accuracy: 0.727410, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 31: Accuracy: 0.725904, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 32: Accuracy: 0.720633, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 33: Accuracy: 0.725904, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 34: Accuracy: 0.719127, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 35: Accuracy: 0.732681, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 36: Accuracy: 0.730422, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 37: Accuracy: 0.731928, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 38: Accuracy: 0.735693, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 39: Accuracy: 0.719880, Feature Subset: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 40: Accuracy: 0.723645, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 41: Accuracy: 0.725904, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 42: Accuracy: 0.728916, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 43: Accuracy: 0.730422, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 44: Accuracy: 0.722892, Feature Subset: [1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 45: Accuracy: 0.732681, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 46: Accuracy: 0.730422, Feature Subset: [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 47: Accuracy: 0.734187, Feature Subset: [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 48: Accuracy: 0.731175, Feature Subset: [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 49: Accuracy: 0.717620, Feature Subset: [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 50: Accuracy: 0.736446, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 51: Accuracy: 0.728916, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 52: Accuracy: 0.727410, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 53: Accuracy: 0.725151, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 54: Accuracy: 0.728163, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 55: Accuracy: 0.728163, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 56: Accuracy: 0.727410, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 57: Accuracy: 0.729669, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 58: Accuracy: 0.716867, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 59: Accuracy: 0.720633, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 60: Accuracy: 0.732681, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 61: Accuracy: 0.733434, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 62: Accuracy: 0.725904, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 63: Accuracy: 0.728916, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 64: Accuracy: 0.731928, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 65: Accuracy: 0.726657, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 66: Accuracy: 0.734187, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 67: Accuracy: 0.724398, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 68: Accuracy: 0.722892, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 69: Accuracy: 0.721386, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Generation 70: Accuracy: 0.722892, Feature Subset: [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Defining a function to run the genetic algorithm (GA) for a specific classifier\n",
    "def run_ga_for_classifier(classifier_name, model):\n",
    "    # Initializing the population of individuals for the GA\n",
    "    population = toolbox.population(n=100)  # Adjusted to match regressor's population size\n",
    "\n",
    "    # Setting up a custom stopping condition\n",
    "    max_stable_generations = 20\n",
    "    stable_generations = 0\n",
    "    previous_best_individual = None\n",
    "    best_individual = None\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    generation = 0\n",
    "    while stable_generations < max_stable_generations:\n",
    "        generation += 1\n",
    "\n",
    "        # Registering the evaluation function with the chosen classifier\n",
    "        toolbox.register(\"evaluate\", evaluate, model=model)\n",
    "\n",
    "        # Running one generation of the genetic algorithm\n",
    "        population, _ = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=1, verbose=False)\n",
    "\n",
    "        # Extracting the best individual of the current generation\n",
    "        current_best_individual = tools.selBest(population, 1)[0]\n",
    "        current_best_accuracy = evaluate(current_best_individual, model)[0]  # Assuming evaluate returns a tuple\n",
    "\n",
    "        # Printing the results of the current best individual\n",
    "        print(f\"Generation {generation}: Accuracy: {current_best_accuracy:.6f}, Feature Subset: {current_best_individual}\")\n",
    "\n",
    "        # Checking if the current best individual is the same as the previous best\n",
    "        if previous_best_individual is not None and current_best_individual == previous_best_individual:\n",
    "            stable_generations += 1\n",
    "        else:\n",
    "            stable_generations = 0\n",
    "            previous_best_individual = current_best_individual\n",
    "\n",
    "        # Updating the best individual if the current one has better accuracy\n",
    "        if current_best_accuracy > best_accuracy:\n",
    "            best_individual = current_best_individual\n",
    "            best_accuracy = current_best_accuracy\n",
    "\n",
    "    # Determining the feature names of the best individual from the final generation\n",
    "    final_selected_features = [i for i, bit in enumerate(best_individual) if bit]\n",
    "    final_selected_feature_names = [feature_names[i] for i in final_selected_features]\n",
    "    final_not_selected_feature_names = [feature_names[i] for i in range(len(feature_names)) if i not in final_selected_features]\n",
    "\n",
    "    return best_individual, best_accuracy, final_selected_features, final_selected_feature_names, final_not_selected_feature_names\n",
    "\n",
    "# Initializing dictionaries to store results for GA-selected features\n",
    "accuracy_ga_features = {}\n",
    "selected_features_ga = {}\n",
    "selected_features_names_ga = {}\n",
    "not_selected_features_names_ga = {}\n",
    "\n",
    "# Running the GA for each classifier and storing results\n",
    "for name, model in classifiers.items():\n",
    "    print(f\"\\nRunning GA for {name}...\")\n",
    "    best_individual, best_accuracy, selected_features, selected_feature_names, not_selected_feature_names = run_ga_for_classifier(name, model)\n",
    "    \n",
    "    # Storing the results for each classifier\n",
    "    accuracy_ga_features[name] = best_accuracy\n",
    "    selected_features_ga[name] = selected_features\n",
    "    selected_features_names_ga[name] = selected_feature_names\n",
    "    not_selected_features_names_ga[name] = not_selected_feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2feb3a1-83d1-4710-8569-136c5c77f350",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">4.7 Final Comparison of MSEs with All Features vs. GA-Selected Features</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5cc7cf31-170c-44dc-b069-a9690e7a5f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Accuracy with All Features vs GA-Selected Features:\n",
      "KNN: Accuracy with All Features: 0.691265 | Accuracy with GA-Selected Features: 0.762048\n",
      "Decision Tree: Accuracy with All Features: 0.660392 | Accuracy with GA-Selected Features: 0.736446\n"
     ]
    }
   ],
   "source": [
    "# Print final comparison of accuracies\n",
    "print(\"\\nComparison of Accuracy with All Features vs GA-Selected Features:\")\n",
    "for name, model in classifiers.items():\n",
    "    # Evaluate model with all features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    all_features_accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    print(f\"{name}: Accuracy with All Features: {all_features_accuracy:.6f} | Accuracy with GA-Selected Features: {accuracy_ga_features[name]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705db67-e20e-4a96-b5bc-5ebe93f035d8",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">Displaying and Tracking Feature Selections After GA Optimization</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "50f5d33c-674d-44fa-89c5-ac9b3068daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features After GA Optimization:\n",
      "KNN:\n",
      "  Features to Select: [3, 8, 11, 16, 18, 19, 21, 22, 23, 24, 27, 30, 31, 32]\n",
      "  Feature names: ['Course', \"Mother's qualification\", \"Father's occupation\", 'Tuition fees up to date', 'Scholarship holder', 'Age at enrollment', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)']\n",
      "  Not Selected Features: ['Marital status', 'Application mode', 'Application order', 'Daytime/evening attendance\\t', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Father's qualification\", \"Mother's occupation\", 'Admission grade', 'Displaced', 'Educational special needs', 'Debtor', 'Gender', 'International', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Unemployment rate', 'Inflation rate', 'GDP']\n",
      "Decision Tree:\n",
      "  Features to Select: [2, 3, 4, 10, 13, 14, 15, 16, 21, 24, 26, 27, 28, 29, 30, 31, 32, 34]\n",
      "  Feature names: ['Application order', 'Course', 'Daytime/evening attendance\\t', \"Mother's occupation\", 'Displaced', 'Educational special needs', 'Debtor', 'Tuition fees up to date', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)', 'Inflation rate']\n",
      "  Not Selected Features: ['Marital status', 'Application mode', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Mother's qualification\", \"Father's qualification\", \"Father's occupation\", 'Admission grade', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (grade)', 'Unemployment rate', 'GDP']\n"
     ]
    }
   ],
   "source": [
    "# Initializing counters to keep track of selected and not selected features\n",
    "selected_feature_counter = Counter()\n",
    "not_selected_feature_counter = Counter()\n",
    "\n",
    "# Printing the final feature selections and non-selections after GA optimization\n",
    "print(\"\\nSelected Features After GA Optimization:\")\n",
    "for name in regressors.keys():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Features to Select: {selected_features_ga[name]}\")\n",
    "    print(f\"  Feature names: {selected_features_names_ga[name]}\")\n",
    "    print(f\"  Not Selected Features: {not_selected_features_names_ga[name]}\")\n",
    "\n",
    "    # Updating the counters with the selected and not selected feature names\n",
    "    selected_feature_counter.update(selected_features_names_ga[name])\n",
    "    not_selected_feature_counter.update(not_selected_features_names_ga[name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a13cf2-d941-4530-9c64-da2d8c40b692",
   "metadata": {},
   "source": [
    "<h2 style=\"color: brown;\">Analyzing Feature Selection Frequency After GA Optimization</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fbbd4b65-0220-4633-9148-356e45806451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Selection Frequency:\n",
      "Selected 2 times: ['Course', 'Tuition fees up to date', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (approved)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)']\n",
      "Selected 1 time: [\"Mother's qualification\", \"Father's occupation\", 'Scholarship holder', 'Age at enrollment', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Application order', 'Daytime/evening attendance\\t', \"Mother's occupation\", 'Displaced', 'Educational special needs', 'Debtor', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Inflation rate']\n",
      "\n",
      "Feature Non-Selection Frequency:\n",
      "Not Selected 2 times: ['Marital status', 'Application mode', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Father's qualification\", 'Admission grade', 'Gender', 'International', 'Curricular units 1st sem (grade)', 'Unemployment rate', 'GDP']\n",
      "Not Selected 1 time: ['Application order', 'Daytime/evening attendance\\t', \"Mother's occupation\", 'Displaced', 'Educational special needs', 'Debtor', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Inflation rate', \"Mother's qualification\", \"Father's occupation\", 'Scholarship holder', 'Age at enrollment', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)']\n"
     ]
    }
   ],
   "source": [
    "# Categorizing the selected features based on how often they are selected\n",
    "selected_2_times = [feature for feature, count in selected_feature_counter.items() if count == 2]\n",
    "selected_1_time = [feature for feature, count in selected_feature_counter.items() if count == 1]\n",
    "\n",
    "# Categorizing the not selected features based on how often they are not selected\n",
    "not_selected_2_times = [feature for feature, count in not_selected_feature_counter.items() if count == 2]\n",
    "not_selected_1_time = [feature for feature, count in not_selected_feature_counter.items() if count == 1]\n",
    "\n",
    "# Printing the categorized results of feature selection frequency\n",
    "print(\"\\nFeature Selection Frequency:\")\n",
    "print(f\"Selected 2 times: {selected_2_times}\")\n",
    "print(f\"Selected 1 time: {selected_1_time}\")\n",
    "\n",
    "# Printing the categorized results of feature non-selection frequency\n",
    "print(\"\\nFeature Non-Selection Frequency:\")\n",
    "print(f\"Not Selected 2 times: {not_selected_2_times}\")\n",
    "print(f\"Not Selected 1 time: {not_selected_1_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2bd98-8c9a-42ca-879c-f7c5054130ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
